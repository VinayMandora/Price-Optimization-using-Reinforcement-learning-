## Price-Optimization-using-Reinforcement-learning

A reinforcement learning framework for dynamic product pricing using Deep Q-Networks (DQN) and Q-Learning.
It learns optimal price adjustments for multiple retail categories â€” Books, Clothing, Electronics, Groceries, Health & Beauty, and Home & Kitchen to maximize profit while staying competitive.

âš™ï¸ Built with TensorFlow / Keras, NumPy, Pandas, and Matplotlib.

**âœ¨ Features**

ğŸ“Š Multi-category dynamic pricing simulation

ğŸ§  Implements both DQN and Tabular Q-Learning

ğŸ” Experience Replay + Epsilon-Greedy exploration

ğŸª„ Reward function penalizes uncompetitive or unchanged prices

ğŸ“ˆ Plots training vs validation loss, epoch rewards, and cumulative rewards

ğŸ’¾ Runs independently on synthetic datasets generated from CSVs


âœ… **Requirements**

Tested on Python 3.10 + TensorFlow 2.x

pandas
numpy
matplotlib
scikit-learn
tensorflow


**Install dependencies:**

pip install -r requirements.txt

âš™ï¸ Setup & Run
**1ï¸âƒ£ Clone and open the folder**
git clone https://github.com/<yourusername>/dynamic-pricing-rl.git
cd dynamic-pricing-rl

**2ï¸âƒ£ (Optional) Create and activate a virtual environment**

python -m venv .venv
.venv\Scripts\activate        # Windows

or
source .venv/bin/activate     # macOS/Linux

**3ï¸âƒ£ Install dependencies**

pip install -r requirements.txt

**4ï¸âƒ£ Run your experiment**

python src/dqn_model.py --dataset data/books.csv --epochs 20
python src/q_learning.py --dataset data/books.csv --epochs 20

**ğŸ§© How It Works**

ğŸ”¹ State Representation

Each product instance encodes:

[Age, Gender, ProductPrice, ProductCost, Profit, FootTraffic,
 InventoryLevel, CompetitorPrice, PurchaseMonth, PurchaseQuarter,
 DayOfWeek, ProductType, NewPrice]

ğŸ”¹ Action Space

Relative price changes such as:

Books / Clothing â†’ Â±5 % or Â±10 %

Electronics â†’ Â±15 % or Â±20 %

Groceries / Health & Beauty â†’ Â±1 % or Â±2 %

ğŸ”¹ Reward Function
reward = (profit - penalty) / 2


Penalties are applied when:

Price â‰ˆ competitor price â†’ âˆ’5

Price â‰ˆ original price â†’ âˆ’3

ğŸ”¹ DQN Model
Dense(64, relu)
Dense(64, relu)
Dense(len(actions), linear)


Optimizer: Adam (lr = 1e-3)
Loss: MSE

ğŸ”¹ Q-Learning Baseline

Implements tabular Q-Learning with learning-rate, discount-factor, and exploration-probability tuning.

**ğŸ“Š Visualization**

Loss Curves â€” Training vs Validation loss per epoch

Epoch Rewards â€” Reward trend across training iterations

Cumulative Rewards â€” Aggregated performance showing agent improvement

Example:

Epoch 1 â†’ Reward â‰ˆ 150
Epoch 20 â†’ Reward â‰ˆ 220 (â†‘ 46%)

**ğŸ§  Insights**

The DQN outperforms tabular Q-Learning on most categories.

Small price-change environments (e.g. Groceries) converge faster.

Larger action spaces (Electronics, Home & Kitchen) yield slower but steady improvement.

**ğŸ§ª Customization**

Tune reward penalties or thresholds.

Modify action magnitudes per category.

Add demand modeling for realistic revenue feedback.

Extend DQN to Double / Dueling / Prioritized Replay variants.

Log experiments using Weights & Biases or MLflow.

**âš ï¸ Notes**

Each row acts as an independent environment (no temporal demand model yet).

TensorFlow may show deprecation warnings (tf.compat.v1.*) â€” safe to ignore.

Replace absolute Windows paths (C:\Users\...) with relative paths (e.g., data/books.csv).

**ğŸ—ºï¸ Roadmap**

 Add revenue = price Ã— predicted demand feedback loop

 Introduce inventory & promotion constraints

 Implement Double DQN / Dueling DQN

 Add CLI & config file support

 Dockerize for reproducibility


**ğŸ‘¤ Author**

Vinay Mandora
